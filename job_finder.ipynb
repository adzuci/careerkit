{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/gist/adzuci/70b705482026a2f52be7b9de3e1a63bd/opt_internship_finder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "readme-top"
   },
   "source": [
    "# Internship & Early-Career Job Scraper (jobspy + Python)\n",
    "\n",
    "This notebook scrapes internship / early-career job postings using **[jobspy](https://github.com/jobspy/jobspy)**\n",
    "and lets you filter them using:\n",
    "\n",
    "- A **visa/OPT-friendly heuristic filter** (optional toggle)\n",
    "- Generic **include / exclude keyword filters**\n",
    "- Simple scoring so you can sort by how well each posting matches your criteria\n",
    "\n",
    "### Credits\n",
    "\n",
    "- **Idea & inspiration:** Vinay Varshigan  \n",
    "- **Debugging & troubleshooting support:** Anja Lee  \n",
    "\n",
    "This notebook is intended to be useful for software engineers and students searching for their dream job.\n",
    "\n",
    "### What this notebook does\n",
    "\n",
    "1. Scrapes recent job postings from multiple sites (via `jobspy`).  \n",
    "2. Normalizes them into a single `pandas` DataFrame.  \n",
    "3. Filters for **internship-like** roles (\"intern\", \"co-op\").  \n",
    "4. Optionally applies **visa/OPT-friendly filters** using keyword heuristics.  \n",
    "5. Optionally applies generic **include / exclude keyword filters**.  \n",
    "6. Computes a simple **match score** and exports results to CSV.\n",
    "\n",
    "### How to use it\n",
    "\n",
    "1. Run the `pip install` cell to install dependencies.  \n",
    "2. Edit the **Search configuration** cell:\n",
    "   - `SEARCH_TERMS`, `SITES`, `LOCATION`, `HOURS_OLD`, etc.\n",
    "   - Set `USE_VISA_FILTERS = True` if you care about OPT/visa heuristics, or `False` if you don't.  \n",
    "   - Optionally set `REQUIRED_KEYWORDS` and `BLOCKED_KEYWORDS` for your own filters.  \n",
    "3. Run all cells top-to-bottom.  \n",
    "4. Open the exported CSV (see the path printed near the end) in your spreadsheet tool of choice.\n",
    "\n",
    "> ⚠️ **Disclaimer**  \n",
    "> - This does **not** guarantee OPT eligibility or visa sponsorship.  \n",
    "> - Always verify details on the company's careers page and with recruiters.  \n",
    "> - Scraping may be subject to each site's Terms of Service; use responsibly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8e5a41b"
   },
   "outputs": [],
   "source": [
    "!pip install -q python-jobspy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09936977"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from jobspy import scrape_jobs\n",
    "\n",
    "def contains_any(text: str, keywords: list[str]) -> bool:\n",
    "    \"\"\"Return True if any keyword appears in the given text (case-insensitive).\"\"\"\n",
    "    if text is None:\n",
    "        return False\n",
    "    text = text.lower()\n",
    "    return any(k in text for k in keywords)\n",
    "\n",
    "def score_text(text: str, good_keywords: list[str], bad_keywords: list[str]) -> int:\n",
    "    \"\"\"Compute a simple matching score: (# good hits) - (# bad hits).\"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    text = text.lower()\n",
    "    score = 0\n",
    "    for k in good_keywords:\n",
    "        score += text.count(k)\n",
    "    for k in bad_keywords:\n",
    "        score -= text.count(k)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ed88cefb"
   },
   "outputs": [],
   "source": [
    "# === Search configuration ===\n",
    "\n",
    "SEARCH_TERMS = [\n",
    "    \"computer science intern\",\n",
    "    \"software engineer intern\",\n",
    "    \"software developer intern\",\n",
    "    \"data science intern\",\n",
    "]\n",
    "\n",
    "# jobspy-supported sites; you can add/remove depending on what works for you\n",
    "SITES = [\n",
    "    \"indeed\",\n",
    "    \"linkedin\",\n",
    "    # \"zip_recruiter\",\n",
    "    # \"glassdoor\",\n",
    "]\n",
    "\n",
    "LOCATION = \"United States\"   # e.g. \"United States\", \"Remote\", \"Boston, MA\"\n",
    "RESULTS_PER_SITE = 150       # number of results per site per search term\n",
    "HOURS_OLD = 168              # limit to last 7 days (168 hours)\n",
    "\n",
    "# === Visa / OPT-friendly keyword heuristics ===\n",
    "# Set USE_VISA_FILTERS = False if you don't care about OPT/visa heuristics\n",
    "\n",
    "USE_VISA_FILTERS = True\n",
    "\n",
    "GOOD_KEYWORDS = [\n",
    "    \"opt\",\n",
    "    \"cpt\",\n",
    "    \"stem opt\",\n",
    "    \"f1\",\n",
    "    \"f-1\",\n",
    "    \"visa sponsorship\",\n",
    "    \"sponsorship available\",\n",
    "    \"sponsor visas\",\n",
    "    \"h-1b\",\n",
    "    \"h1b\",\n",
    "    \"international students\",\n",
    "]\n",
    "\n",
    "BAD_KEYWORDS = [\n",
    "    \"us citizens only\",\n",
    "    \"u.s. citizens only\",\n",
    "    \"must be a us citizen\",\n",
    "    \"citizen only\",\n",
    "    \"no sponsorship\",\n",
    "    \"cannot sponsor\",\n",
    "    \"unable to sponsor\",\n",
    "    \"not provide sponsorship\",\n",
    "    \"gc or citizen only\",\n",
    "    \"green card or citizen only\",\n",
    "]\n",
    "\n",
    "# === Generic filters (always available) ===\n",
    "# Use these for non-visa-specific use cases. Leave empty ([]) to ignore.\n",
    "\n",
    "REQUIRED_KEYWORDS: list[str] = []  # e.g. [\"machine learning\", \"python\"]\n",
    "BLOCKED_KEYWORDS: list[str] = []   # e.g. [\"unpaid\", \"commission only\"]\n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0d1350fe"
   },
   "outputs": [],
   "source": [
    "all_jobs = []\n",
    "\n",
    "for site in SITES:\n",
    "    for term in SEARCH_TERMS:\n",
    "        print(f\"Scraping {site} for '{term}' in {LOCATION} (last {HOURS_OLD} hours)...\")\n",
    "        try:\n",
    "            jobs_df = scrape_jobs(\n",
    "                site_name=site,\n",
    "                search_term=term,\n",
    "                location=LOCATION,\n",
    "                results_wanted=RESULTS_PER_SITE,\n",
    "                hours_old=HOURS_OLD,\n",
    "                country_indeed=\"USA\",  # relevant for Indeed\n",
    "            )\n",
    "            jobs_df[\"site\"] = site\n",
    "            jobs_df[\"search_term\"] = term\n",
    "            all_jobs.append(jobs_df)\n",
    "            print(f\"  -> Retrieved {len(jobs_df)} results.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  !! Error scraping {site} for '{term}': {e}\")\n",
    "\n",
    "if not all_jobs:\n",
    "    raise RuntimeError(\"No jobs retrieved. Try changing sites, search terms, or HOURS_OLD.\")\n",
    "\n",
    "raw_df = pd.concat(all_jobs, ignore_index=True)\n",
    "print(f\"\\nTotal raw jobs collected: {len(raw_df)}\")\n",
    "\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7dc9312"
   },
   "outputs": [],
   "source": [
    "df = raw_df.copy()\n",
    "\n",
    "# Create a combined text field to search for keywords\n",
    "description_col = \"description\" if \"description\" in df.columns else None\n",
    "snippet_col = \"snippet\" if \"snippet\" in df.columns else None\n",
    "\n",
    "text_parts = [df[\"title\"].fillna(\"\").astype(str)]\n",
    "if description_col:\n",
    "    text_parts.append(df[description_col].fillna(\"\").astype(str))\n",
    "if snippet_col:\n",
    "    text_parts.append(df[snippet_col].fillna(\"\").astype(str))\n",
    "\n",
    "df[\"search_text\"] = text_parts[0]\n",
    "for part in text_parts[1:]:\n",
    "    df[\"search_text\"] = df[\"search_text\"] + \" \" + part\n",
    "\n",
    "df[\"search_text\"] = df[\"search_text\"].str.lower()\n",
    "\n",
    "# Filter for internships explicitly (job title contains \"intern\" or \"co-op\")\n",
    "intern_mask = df[\"title\"].str.lower().str.contains(\"intern|co-op|co op\", na=False)\n",
    "\n",
    "df_interns = df[intern_mask].copy()\n",
    "print(f\"Internship-like roles: {len(df_interns)}\")\n",
    "\n",
    "df_interns[[\"title\", \"company\", \"location\", \"site\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1f2e82e2"
   },
   "outputs": [],
   "source": [
    "# Apply visa/OPT heuristics (optional) and generic include/exclude filters\n",
    "\n",
    "mask = pd.Series(True, index=df_interns.index)\n",
    "\n",
    "# Visa / OPT-friendly filter (toggle)\n",
    "if USE_VISA_FILTERS:\n",
    "    visa_good_mask = df_interns[\"search_text\"].apply(lambda t: contains_any(t, GOOD_KEYWORDS))\n",
    "    visa_bad_mask = df_interns[\"search_text\"].apply(lambda t: contains_any(t, BAD_KEYWORDS))\n",
    "    mask &= visa_good_mask & ~visa_bad_mask\n",
    "\n",
    "# Generic required keywords\n",
    "if REQUIRED_KEYWORDS:\n",
    "    required_mask = df_interns[\"search_text\"].apply(lambda t: contains_any(t, REQUIRED_KEYWORDS))\n",
    "    mask &= required_mask\n",
    "\n",
    "# Generic blocked keywords\n",
    "if BLOCKED_KEYWORDS:\n",
    "    blocked_mask = df_interns[\"search_text\"].apply(lambda t: contains_any(t, BLOCKED_KEYWORDS))\n",
    "    mask &= ~blocked_mask\n",
    "\n",
    "df_opt = df_interns[mask].copy()\n",
    "print(f\"Roles after filtering: {len(df_opt)}\")\n",
    "\n",
    "# Add a 'match_score' column for simple ranking\n",
    "if USE_VISA_FILTERS:\n",
    "    good_for_score = GOOD_KEYWORDS\n",
    "    bad_for_score = BAD_KEYWORDS\n",
    "else:\n",
    "    good_for_score = REQUIRED_KEYWORDS\n",
    "    bad_for_score = BLOCKED_KEYWORDS\n",
    "\n",
    "if good_for_score or bad_for_score:\n",
    "    df_opt[\"match_score\"] = df_opt[\"search_text\"].apply(\n",
    "        lambda t: score_text(t, good_for_score, bad_for_score)\n",
    "    )\n",
    "else:\n",
    "    # If no keyword lists are provided, default match_score to 0\n",
    "    df_opt[\"match_score\"] = 0\n",
    "\n",
    "# Select useful columns if they exist\n",
    "columns_to_keep = []\n",
    "for col in [\"title\", \"company\", \"location\", \"site\", \"search_term\", \"url\", \"description\", \"snippet\", \"match_score\"]:\n",
    "    if col in df_opt.columns:\n",
    "        columns_to_keep.append(col)\n",
    "\n",
    "df_opt = df_opt[columns_to_keep]\n",
    "\n",
    "# Sort by match_score descending (higher = more matches)\n",
    "df_opt = df_opt.sort_values(\"match_score\", ascending=False)\n",
    "\n",
    "df_opt.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46b27b8",
   "metadata": {
    "id": "9798f84a"
   },
   "outputs": [],
   "source": [
    "# Deduplicate based on title + company + location + site\n",
    "dedupe_keys = [c for c in [\"title\", \"company\", \"location\", \"site\"] if c in df_opt.columns]\n",
    "df_opt_unique = df_opt.drop_duplicates(subset=dedupe_keys, keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "print(f\"After deduplication: {len(df_opt_unique)} roles\\n\")\n",
    "\n",
    "# Show a preview\n",
    "preview_cols = [c for c in [\"title\", \"company\", \"location\", \"site\", \"job_url_direct\", \"match_score\"] if c in df_opt_unique.columns]\n",
    "df_opt_unique[preview_cols].head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0678fb6"
   },
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "output_path = \"filtered_internships.csv\"\n",
    "df_opt_unique.to_csv(output_path, index=False)\n",
    "print(f\"Saved {len(df_opt_unique)} roles to: {output_path}\")\n",
    "print(f\"Visa/OPT filters {'ENABLED' if USE_VISA_FILTERS else 'DISABLED'}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf32d458"
   },
   "outputs": [],
   "source": [
    "print(\"=== Summary ===\")\n",
    "print(f\"Total raw jobs scraped: {len(raw_df)}\")\n",
    "print(f\"Internship-like roles: {len(df_interns)}\")\n",
    "print(f\"Roles after filtering (before dedupe): {len(df_opt)}\")\n",
    "print(f\"Unique roles after dedupe: {len(df_opt_unique)}\")\n",
    "\n",
    "if USE_VISA_FILTERS:\n",
    "    print(\"\\nVisa/OPT filters were ENABLED.\")\n",
    "else:\n",
    "    print(\"\\nVisa/OPT filters were DISABLED (generic filtering only).\")\n",
    "\n",
    "print(\"\\nSample of results:\")\n",
    "sample_cols = [c for c in [\"title\", \"company\", \"location\", \"site\", \"match_score\"] if c in df_opt_unique.columns]\n",
    "df_opt_unique[sample_cols].head(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
